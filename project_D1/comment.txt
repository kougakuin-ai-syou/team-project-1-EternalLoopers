簡易なグリッドワールド環境において Q学習による方策学習を実装する

ねらい：
関数的実装	全体を関数で構成。OOPとの違いを明示
Qテーブル	状態-行動ペアに基づいた Q学習
ε-greedy	探索と活用のバランス
状態遷移と報酬	グリッド上の移動と報酬計算の簡素なモデル
学習とテストの分離	train() と test() 関数を分けて学習と実行を明確に区別

拡張（案）：
ステップ数をカウントして評価指標に追加
壁 (wall_state) やトラップ (trap_state) を追加
matplotlib を使って Q値や経路を視覚化